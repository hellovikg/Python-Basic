{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQgUTzuHf2Hg"
      },
      "outputs": [],
      "source": [
        "# Question 1:  What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "\"\"\"\n",
        "A Decision Tree is a supervised machine learning algorithm that can be used for both classification and regression tasks. It works by recursively partitioning the data into smaller subsets based on the values of input features. The goal is to create a tree-like model of decisions and their possible consequences.\n",
        "\n",
        "In the context of classification, a Decision Tree works as follows:\n",
        "\n",
        "Splitting the Data: The algorithm starts with the entire dataset at the root node. It then evaluates different features and their possible split points to find the one that best separates the data into distinct classes. The 'best' split is typically determined by metrics like Gini impurity or entropy, which measure the homogeneity of the classes within each subset.\n",
        "\n",
        "Creating Nodes: Based on the chosen split, the data is divided into two or more subsets, and child nodes are created for each subset. This process is repeated recursively for each child node, essentially building the tree downwards.\n",
        "\n",
        "Decision Rules: Each internal node in the tree represents a test on an attribute (feature), and each branch represents the outcome of that test. For example, a node might test if 'Age > 30'.\n",
        "\n",
        "Leaf Nodes: The recursion stops when a node contains data that is sufficiently 'pure' (i.e., mostly belongs to a single class), or when other stopping criteria are met (e.g., maximum depth of the tree, minimum number of samples per leaf). These final nodes are called leaf nodes, and each leaf node represents a class label or a probability distribution over the classes.\n",
        "\n",
        "Classification: To classify a new data point, you start at the root of the tree and traverse down the branches by answering the questions at each internal node based on the data point's feature values. Eventually, you reach a leaf node, and the class associated with that leaf node is the predicted class for the new data point.\n",
        "\n",
        "Key Characteristics:\n",
        "\n",
        "Interpretability: Decision trees are easy to understand and interpret, as their structure mirrors human decision-making.\n",
        "Non-parametric: They don't make assumptions about the underlying distribution of the data.\n",
        "Handles both numerical and categorical data: They can work with various data types.\n",
        "Prone to overfitting: Without proper pruning or limiting the tree's depth, decision trees can easily overfit the training data, leading to poor generalization on unseen data.\n",
        "Sensitivity to data variations: Small changes in the data can lead to a completely different tree structure.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Gini Impurity and Entropy are two common metrics used in Decision Tree algorithms to measure the 'impurity' or 'disorder' of a set of samples.\n",
        "The goal of a Decision Tree is to make splits that reduce this impurity as much as possible, leading to more homogeneous child nodes.\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "p2YdmEGUgeLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "\"\"\"\n",
        "Pre-Pruning and Post-Pruning are two techniques used to prevent Decision Trees from overfitting the training data by reducing the complexity of the tree.\n",
        "The main difference between Pre-Pruning and Post-Pruning in Decision Trees lies in when the pruning occurs and how it affects the tree-building process:\n",
        "\n",
        "Pre-Pruning (Early Stopping): This technique stops the tree's growth during its construction.\n",
        "It uses criteria like maximum depth, minimum samples per split, or minimum impurity reduction to decide whether to continue splitting a node.\n",
        "Its advantage is reduced training time and computational cost.\n",
        "\n",
        "Post-Pruning (Backward Pruning): This technique involves growing a complete (or nearly complete) Decision Tree first, and then afterwards, it removes branches or nodes from the fully grown tree.\n",
        "It evaluates subtrees to see if their removal improves performance on a validation set or simplifies the model.\n",
        "Its advantage is that it often leads to more optimal or better-performing trees on unseen data.\n",
        "\n",
        "\n",
        "The practical advantages for each are:\n",
        "\n",
        "Pre-Pruning: A significant advantage is that it reduces the training time and computational cost by stopping the tree growth early.\n",
        "\n",
        "Post-Pruning: A practical advantage is that it often leads to more optimal or better-performing trees on unseen data because it allows the tree to explore all splits before removing those that don't generalize well.\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7qPK4eQTgw54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "\"\"\"\n",
        "Information Gain is a crucial concept in Decision Trees, especially for classification tasks.\n",
        "It quantifies how much the uncertainty (entropy) of the target variable decreases after splitting the data based on a particular feature. Essentially, it helps the Decision Tree choose the best split.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZLhsBSxqhyfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Common Real-World Applications: Decision Trees are used in various fields like medical diagnosis, credit scoring and fraud detection, customer relationship management (CRM), manufacturing quality control, and bioinformatics.\n",
        "\n",
        "Main Advantages: They are highly interpretable and explainable, can handle both numerical and categorical data without extensive pre-processing, do not require data scaling, are non-parametric, and are relatively robust to outliers.\n",
        "\n",
        "Main Limitations: They are prone to overfitting without proper pruning, can be unstable and exhibit high variance, may show bias towards dominant classes in imbalanced datasets, and their greedy approach to splitting may not lead to globally optimal trees.\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "6OF0bbusiEo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Dataset Info:\n",
        "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "● Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "\n",
        "\n",
        "\n",
        "Question 6:   Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "XI7OWGxaitVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 1. Train a Decision Tree Classifier with max_depth=3\n",
        "dtc_max_depth_3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "dtc_max_depth_3.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy for max_depth=3\n",
        "y_pred_3 = dtc_max_depth_3.predict(X_test)\n",
        "accuracy_3 = accuracy_score(y_test, y_pred_3)\n",
        "print(f\"Accuracy for Decision Tree with max_depth=3: {accuracy_3:.4f}\")\n",
        "\n",
        "# 2. Train a fully-grown Decision Tree Classifier (default max_depth=None)\n",
        "dtc_fully_grown = DecisionTreeClassifier(random_state=42)\n",
        "dtc_fully_grown.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy for fully-grown tree\n",
        "y_pred_full = dtc_fully_grown.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "print(f\"Accuracy for fully-grown Decision Tree: {accuracy_full:.4f}\")\n",
        "\n",
        "print(\"\\nComparison:\")\n",
        "if accuracy_3 > accuracy_full:\n",
        "    print(\"The Decision Tree with max_depth=3 performed better.\")\n",
        "elif accuracy_full > accuracy_3:\n",
        "    print(\"The fully-grown Decision Tree performed better.\")\n",
        "else:\n",
        "    print(\"Both Decision Trees performed equally well.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMvzTU-2i9v0",
        "outputId": "1cce4e2c-b988-4518-efae-b9fd1156518e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Decision Tree with max_depth=3: 1.0000\n",
            "Accuracy for fully-grown Decision Tree: 1.0000\n",
            "\n",
            "Comparison:\n",
            "Both Decision Trees performed equally well.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Question 7:   Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Here's a Python program to load the Iris Dataset, train two Decision Tree Classifiers (one with max_depth=3 and another fully grown), and compare their accuracies.\n",
        "The code will be added to the notebook.\n",
        "\n",
        "The Python program successfully loaded the Iris Dataset and trained two Decision Tree Classifiers.\n",
        "Both the tree with max_depth=3 and the fully-grown tree achieved an accuracy of 1.0000 on the test set, indicating perfect classification for this particular split of the Iris dataset.\n",
        "As a result, the comparison shows that \"Both Decision Trees performed equally well.\"\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "SMT2JsPgjsW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question 8: Write a Python program to:\n",
        "● Load the Boston Housing Dataset\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Vsu5V7grkdu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fa8bacb",
        "outputId": "1d99c204-22d5-4763-c1b4-9ac9148166ca"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing Dataset as an alternative\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Convert to DataFrame for better feature name handling\n",
        "X_df = pd.DataFrame(X, columns=housing.feature_names)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "dtr = DecisionTreeRegressor(random_state=42)\n",
        "dtr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dtr.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(X_df.columns, dtr.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.5280\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5235\n",
            "HouseAge: 0.0521\n",
            "AveRooms: 0.0494\n",
            "AveBedrms: 0.0250\n",
            "Population: 0.0322\n",
            "AveOccup: 0.1390\n",
            "Latitude: 0.0900\n",
            "Longitude: 0.0888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question 9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],  # None means full depth\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
        "\n",
        "# Get the best model\n",
        "best_dt_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set with the best model\n",
        "y_pred_best = best_dt_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the best model\n",
        "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "print(f\"Accuracy with best parameters: {accuracy_best:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OaUwJBRkrEo",
        "outputId": "8dca9003-f56c-4731-d1ef-2bc5b791fc61"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Accuracy with best parameters: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "\n",
        "Handling Missing Values: This includes identifying missing data, and then choosing appropriate imputation strategies like dropping rows/columns, mean/median/mode imputation, regression imputation, or KNN imputation, often guided by domain knowledge.\n",
        "\n",
        "Encoding Categorical Features: Convert non-numerical categorical data into a numerical format that the Decision Tree can process. Common methods include One-Hot Encoding for nominal features and Ordinal Encoding for ordered features.\n",
        "\n",
        "Training a Decision Tree Model: Split the preprocessed data into training and testing sets, then initialize and fit a DecisionTreeClassifier to the training data.\n",
        "\n",
        "Tuning its Hyperparameters: Crucial for preventing overfitting. This involves using techniques like GridSearchCV or RandomizedSearchCV with cross-validation to find the optimal values for hyperparameters such as max_depth, min_samples_split, and min_samples_leaf.\n",
        "\n",
        "Evaluating its Performance: Assess the best-tuned model's performance on the unseen test set using metrics appropriate for classification, such as accuracy, precision, recall, F1-score, ROC AUC, and analyzing the confusion matrix.\n",
        "\n",
        "From a business value perspective, this model could provide:\n",
        "\n",
        "Early Detection and Intervention: Leading to better patient outcomes and potentially reduced treatment costs.\n",
        "Resource Optimization: Efficient allocation of healthcare resources by identifying high-risk patients.\n",
        "Cost Reduction: Preventing disease progression to more expensive stages.\n",
        "Personalized Medicine: Informing tailored prevention and treatment plans.\n",
        "Improved Patient Satisfaction: Through more accurate diagnoses and timely care.\n",
        "Research and Development: Providing insights into disease mechanisms and risk factors.\n",
        "Proactive Healthcare: Shifting towards a preventive approach rather than a reactive one.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "flF7KmcJlBAb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}