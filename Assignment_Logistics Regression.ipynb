{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oOX4yTEAVfSj"
      },
      "outputs": [],
      "source": [
        "# Question 1 : What is Simple Linear Regression (SLR)? Explain its purpose.\n",
        "\n",
        "\"\"\"\n",
        "Simple Linear Regression (SLR) is a statistical method used to model the relationship between two continuous variables: a dependent variable (also known as the response or outcome variable) and an independent variable (also known as the predictor or explanatory variable).\n",
        "\n",
        "Predict: Estimate the value of the dependent variable based on the value of the independent variable.\n",
        "Explain: Understand the strength and direction of the relationship between the two variables. For example, if the independent variable increases, does the dependent variable tend to increase, decrease, or stay the same, and by how much?\n",
        "Quantify: Provide a mathematical equation (a straight line) that describes this relationship, typically in the form Y = β₀ + β₁X + ε, where:\n",
        "Y is the dependent variable.\n",
        "X is the independent variable.\n",
        "β₀ is the y-intercept (the value of Y when X is 0).\n",
        "β₁ is the slope of the line (the change in Y for a one-unit change in X).\n",
        "ε is the error term, representing the difference between the observed and predicted values.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 2: What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "The key assumptions of Simple Linear Regression are:\n",
        "\n",
        "Linearity: There is a linear relationship between the independent variable (X) and the dependent variable (Y). This means that the relationship can be best described by a straight line.\n",
        "Independence of Errors: The residuals (errors) are independent of each other. This means that the error of one observation does not influence the error of another.\n",
        "Homoscedasticity: The variance of the residuals is constant across all levels of the independent variable. In simpler terms, the spread of the residuals should be roughly the same along the regression line.\n",
        "Normality of Errors: The residuals are normally distributed. While less critical for larger sample sizes due to the Central Limit Theorem, it's an important assumption, especially for making inferences.\n",
        "No Multicollinearity (for Multiple Linear Regression): Although not strictly an assumption for Simple Linear Regression (as there's only one independent variable), it's worth noting for future reference when dealing with multiple predictors: independent variables should not be highly correlated with each other.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QLswFvF5XC6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 3: Write the mathematical equation for a simple linear regression model and explain each term.\n",
        "\n",
        "\"\"\"\n",
        "The mathematical equation for a simple linear regression model is typically represented as a straight line:\n",
        "\n",
        "Y = β₀ + β₁X + ε\n",
        "\n",
        "Here's an explanation of each term:\n",
        "\n",
        "Y: This is the dependent variable (also known as the response or outcome variable). It's the variable we are trying to predict or explain.\n",
        "X: This is the independent variable (also known as the predictor or explanatory variable). It's the variable used to predict Y.\n",
        "β₀ (Beta-naught): This is the y-intercept. It represents the expected value of Y when the independent variable X\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dnqFljsVXCrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " ## Question 4: Provide a real-world example where simple linear regression can be applied.\n",
        "\n",
        "\n",
        " \"\"\"\n",
        "A common real-world example where Simple Linear Regression can be applied is in predicting house prices based on their size.\n",
        "\n",
        "Dependent Variable (Y): House Price (e.g., in dollars)\n",
        "Independent Variable (X): House Size (e.g., in square feet)\n",
        "In this scenario, a real estate analyst could collect data on various houses, noting both their size and their sale price. By applying Simple Linear Regression, they could:\n",
        "\n",
        "Understand the relationship: Determine if larger houses generally sell for higher prices.\n",
        "Quantify the relationship: Find an equation that describes how much the price typically increases for every additional square foot.\n",
        "Predict: Estimate the selling price of a new house based solely on its square footage. For example, if the regression model suggests Price = $50,000 + $150 * Size (sq ft), a 2000 sq ft house would be predicted to sell for $50,000 + $150 * 2000 = $350,000.\n",
        "\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "BcFvV06tb7o6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Question 5: What is the method of least squares in linear regression?\n",
        "\"\"\"\n",
        "The Method of Least Squares is a standard approach in linear regression to find the best-fitting straight line (the regression line) through a set of data points. The 'best-fitting' line is defined as the one that minimizes the sum of the squares of the vertical distances (residuals) from each data point to the line.\n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "Residuals (Errors): For each data point (Xᵢ, Yᵢ), a residual is the difference between the observed actual value (Yᵢ) and the value predicted by the regression line (Ŷᵢ). That is, eᵢ = Yᵢ - Ŷᵢ.\n",
        "Squaring the Residuals: We square each residual (eᵢ²) for two main reasons:\n",
        "To prevent positive and negative errors from canceling each other out, ensuring that larger errors have a greater impact.\n",
        "To penalize larger errors more heavily, making the model more sensitive to deviations.\n",
        "Sum of Squared Residuals (SSR): The goal is to minimize the sum of all these squared residuals: SSR = Σ(Yᵢ - Ŷᵢ)².\n",
        "Finding the Best Line: The method of least squares mathematically derives the values for the intercept (β₀) and the slope (β₁) of the regression line Ŷ = β₀ + β₁X that result in the smallest possible SSR.\n",
        "In essence, the method of least squares provides a systematic way to draw a line through a scatter plot of data points such that the average squared distance between the points and the line is as small as possible. This line then serves as the best linear predictor for the relationship between the independent and dependent variables.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MrtVYl5zsCsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Question 6: What is Logistic Regression? How does it differ from Linear Regression?\n",
        "\n",
        "\"\"\"\n",
        "Logistic Regression is a statistical model used for binary classification problems. It predicts the probability of a categorical dependent variable (an outcome that can only take on one of a limited number of categories, e.g., 'yes' or 'no', 'true' or 'false'). Instead of directly predicting a value, it predicts the probability that an observation belongs to a particular category.\n",
        "\n",
        "Here's how it differs from Linear Regression:\n",
        "\n",
        "Dependent Variable Type: Linear Regression is used when the dependent variable is continuous (e.g., house price, temperature). Logistic Regression is used when the dependent variable is categorical, typically binary (e.g., whether a customer will churn or not, whether an email is spam or not).\n",
        "Output: Linear Regression outputs a continuous numerical value. Logistic Regression outputs a probability value between 0 and 1, which can then be converted into a binary outcome using a threshold.\n",
        "Underlying Function: Linear Regression uses a linear function to model the relationship between variables (Y = β₀ + β₁X). Logistic Regression uses the sigmoid (or logistic) function to transform the linear combination of predictors into a probability. The sigmoid function maps any real-valued number into a value between 0 and 1.\n",
        "Error Distribution: Linear Regression assumes that the residuals are normally distributed. Logistic Regression typically assumes a binomial distribution for the errors, as it deals with binary outcomes.\n",
        "Equation: The basic equation for Logistic Regression often looks like this (after applying the log-odds transformation): log(p / (1-p)) = β₀ + β₁X where p is the probability of the event occurring.\n",
        "In essence, while both are regression techniques, Linear Regression predicts quantity, and Logistic Regression predicts the probability of belonging to a class.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZkwoaZATsOdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Question 7: Name and briefly describe three common evaluation metrics for regression models.\n",
        "\n",
        "\"\"\"\n",
        "Mean Absolute Error (MAE): To measure the average error in price predictions.\n",
        "Root Mean Squared Error (RMSE): To emphasize larger errors, which are critical in financial applications.\n",
        "R-squared (R²): To assess how well the model captures the variability in stock prices.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "TPkyDaIWsjmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Question 8: What is the purpose of the R-squared metric in regression analysis?\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "The purpose of the R-squared metric in regression analysis is to indicate how well the independent variables explain the variance in the dependent variable.\n",
        "Also known as the coefficient of determination, R-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variables\n",
        ", serving as a measure of the goodness of fit for the regression model.\n",
        "A higher R-squared value means that more of the variance is explained by the model.\n",
        "Measure of fit: R-squared quantifies how closely the data points fit the regression line.\n",
        "\n",
        "Proportion of variance: It represents the percentage of the dependent variable's variation that is accounted for by the independent variable(s) in the model.\n",
        "For example, an R-squared of \\(0.75\\) means that \\(75\\%\\) of the variation in the dependent variable can be explained by the model.\n",
        "\n",
        "Goodness of fit: A value of \\(1\\) means the model perfectly predicts the dependent variable\n",
        ", while a value of \\(0\\) means the model does not explain any of the variability.\n",
        "\n",
        "Context is key: It's important to note that R-squared should be interpreted alongside other statistical measures and context\n",
        ", as a high R-squared does not automatically mean the model is causal or the best possible model.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "aaxDuH_ltCvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##  Question 9: Write Python code to fit a simple linear regression model using scikit-learn and print the slope and intercept.\n",
        "## (Include your Python code and output in the code box below.)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# 1. Create some sample data\n",
        "# Let's say we want to predict 'study_hours' (Y) based on 'exam_score' (X)\n",
        "X = np.array([2, 3, 5, 7, 9, 10, 12, 14, 15, 17, 18, 20]).reshape(-1, 1) # Independent variable (e.g., hours studied)\n",
        "Y = np.array([55, 60, 65, 70, 75, 80, 85, 88, 90, 92, 95, 98]) # Dependent variable (e.g., exam score)\n",
        "\n",
        "# 2. Initialize the Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# 3. Fit the model to the data\n",
        "model.fit(X, Y)\n",
        "\n",
        "# 4. Print the slope (coefficient) and intercept\n",
        "print(f\"Intercept: {model.intercept_:.2f}\")\n",
        "print(f\"Slope (Coefficient): {model.coef_[0]:.2f}\")\n",
        "\n",
        "# Optional: Make a prediction\n",
        "sample_study_hours = np.array([[13]])\n",
        "predicted_score = model.predict(sample_study_hours)\n",
        "print(f\"\\nPredicted exam score for {sample_study_hours[0][0]} hours studied: {predicted_score[0]:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiCg-mL9uJav",
        "outputId": "af420dc2-0fb0-454f-f9bc-8f8df3ef0eb4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: 53.37\n",
            "Slope (Coefficient): 2.37\n",
            "\n",
            "Predicted exam score for 13 hours studied: 84.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Question 10: How do you interpret the coefficients in a simple linear regression model?\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "In a simple linear regression model, the coefficients represent the relationship between the independent and dependent variables.\n",
        "The slope coefficient indicates how much the dependent variable changes, on average, for a one-unit increase in the independent variable.\n",
        "The sign of the slope (\\(\\beta _{1}\\)) shows the direction of the relationship (positive or negative), while the magnitude indicates the strength of that change.\n",
        "The intercept (\\(\\beta _{0}\\)) is the predicted value of the dependent variable when the independent variable is zero, though it often has no practical meaning.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "esuy4Hphu5o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b9Rgz8Vju5bn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}